{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "import operator\n",
    "import numpy as np\n",
    "from scipy.special import betaln\n",
    "from random import shuffle\n",
    "from collections import defaultdict\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 3\n",
    "if dataset==0:\n",
    "    # MovieLens-1M dataset\n",
    "    ratings_file = '../data/ml-1m/ratings.dat'\n",
    "    delimiter = '::'\n",
    "elif dataset==1:\n",
    "    # MovieLens-100k dataset\n",
    "    ratings_file = '../data/ml-100k/u.data'\n",
    "    delimiter = '\\t'\n",
    "elif dataset==2:\n",
    "    # MovieLens-20k dataset\n",
    "    ratings_file = '../data/ml-20k/u20k.data'\n",
    "    delimiter = '\\t'\n",
    "elif dataset==3:\n",
    "    ratings_file = '../data/filmtrust/ratings.txt'\n",
    "    delimiter = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    ratings = open(ratings_file, 'r').read().split('\\n')[:-1]\n",
    "    user_item_map = {}\n",
    "    for r in ratings:\n",
    "        attrs = r.split(delimiter)\n",
    "        #if len(attrs) < 4:\n",
    "        #    continue\n",
    "        user = 'u' + attrs[0]\n",
    "        item = 'i' + attrs[1]\n",
    "        rating = float(attrs[2])\n",
    "        if user in user_item_map:\n",
    "            user_item_map[user][item] = rating\n",
    "        else:\n",
    "            user_item_map[user] = {}\n",
    "            user_item_map[user][item] = rating\n",
    "    for user in user_item_map:\n",
    "        sum = 0\n",
    "        for item in user_item_map[user]:\n",
    "            sum += user_item_map[user][item]\n",
    "        avg_rating_user = sum * 1.0 / len(user_item_map[user])\n",
    "        for item in user_item_map[user]:\n",
    "            if user_item_map[user][item] >= avg_rating_user:\n",
    "                user_item_map[user][item] = 1\n",
    "            else:\n",
    "                user_item_map[user][item] = 0\n",
    "    #for user in user_item_map.keys():\n",
    "    #    if len(user_item_map[user]) < 10:\n",
    "    #        del user_item_map[user]\n",
    "    return user_item_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def form_graph(user_item_map):\n",
    "    graph = {}\n",
    "    for user in user_item_map:\n",
    "        if user not in graph:\n",
    "            graph[user] = set([])\n",
    "        for item in user_item_map[user]:\n",
    "            if item not in graph:\n",
    "                graph[item] = set([])\n",
    "            graph[user].add(item)\n",
    "            graph[item].add(user)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_graph(graph):\n",
    "    while True:\n",
    "        changed = False\n",
    "        delete_nodes = []\n",
    "        for node in graph:\n",
    "            if len(graph[node]) < 10:\n",
    "                changed = True\n",
    "                delete_nodes.append(node)\n",
    "        for node in delete_nodes:\n",
    "            del graph[node]\n",
    "        for node1 in graph:\n",
    "            delete_nodes = []\n",
    "            for node2 in graph[node1]:\n",
    "                if node2 not in graph:\n",
    "                    changed = True\n",
    "                    delete_nodes.append(node2)\n",
    "            for node2 in delete_nodes:\n",
    "                graph[node1].remove(node2)\n",
    "        if not changed:\n",
    "            break\n",
    "    for node in graph:\n",
    "        graph[node] = list(graph[node])\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_ratings(user_item_map, graph):\n",
    "    item_rating_map = {}\n",
    "    for user in user_item_map:\n",
    "        if user not in graph:\n",
    "            continue\n",
    "        for item in user_item_map[user]:\n",
    "            if item not in graph:\n",
    "                continue\n",
    "            if item not in item_rating_map:\n",
    "                item_rating_map[item] = [1, 1]\n",
    "            if user_item_map[user][item] == 0:\n",
    "                item_rating_map[item][1] += 1\n",
    "            else:\n",
    "                item_rating_map[item][0] += 1\n",
    "    return item_rating_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_item_map = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = form_graph(user_item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = clean_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_rating_map = get_num_ratings(user_item_map, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings: 28317\n",
      "Number of users: 949\n",
      "Number of items: 159\n",
      "Delta_U: 83\n",
      "Delta_I: 848\n",
      "Gamma_U: 29.8387776607\n",
      "Gamma_I: 178.094339623\n"
     ]
    }
   ],
   "source": [
    "R = 0\n",
    "U = 0\n",
    "I = 0\n",
    "delta_u = 0\n",
    "delta_i = 0\n",
    "total_i = 0\n",
    "\n",
    "for u in graph:\n",
    "    if 'i' in u:\n",
    "        I += 1\n",
    "        if len(graph[u]) > delta_i:\n",
    "            delta_i = len(graph[u])\n",
    "        total_i += len(graph[u])\n",
    "        continue\n",
    "    U += 1\n",
    "    R += len(graph[u])\n",
    "    if len(graph[u]) > delta_u:\n",
    "        delta_u = len(graph[u])\n",
    "    \n",
    "print 'Number of ratings: ' + str(R)\n",
    "print 'Number of users: ' + str(U)\n",
    "print 'Number of items: ' + str(I)\n",
    "print 'Delta_U: ' + str(delta_u)\n",
    "print 'Delta_I: ' + str(delta_i)\n",
    "print 'Gamma_U: ' + str(R * 1.0 / U)\n",
    "print 'Gamma_I: ' + str(total_i * 1.0 / I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset so that for each user, 80% is training, 20% is for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i9', 'i8']\n"
     ]
    }
   ],
   "source": [
    "first_20percent_of_u1 = [graph['u1'][i] for i in range(int(0.2*len(graph['u1'])))] \n",
    "print first_20percent_of_u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph['u1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph['u1'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: a graph called g\n",
    "# Output: two test sets, one containing 20% of g's items per user, \n",
    "#         the other containing 80% \n",
    "\n",
    "def create_sets(g): \n",
    "    test_set = defaultdict(list) \n",
    "    training_set = defaultdict(list)\n",
    "    \n",
    "    for key in g:\n",
    "        if key[0]=='u': \n",
    "            indices = [x for x in xrange(len(g[key]))]\n",
    "            shuffle(indices)\n",
    "            # For length of items belonging to that key, split 20% and 80%\n",
    "            first_80 = int(0.8*len(g[key]))\n",
    "            \n",
    "            for i in range(first_80): \n",
    "                training_set[key].append(g[key][indices[i]])\n",
    "\n",
    "            for j in range(first_80, len(g[key])):\n",
    "                test_set[key].append(g[key][indices[j]])\n",
    "            \n",
    "    for user in test_set.keys():\n",
    "        for item in test_set[user]:\n",
    "            test_set[item].append(user)\n",
    "            \n",
    "    for user in training_set.keys(): \n",
    "        for item in training_set[user]: \n",
    "            training_set[item].append(user)\n",
    "            \n",
    "    return test_set, training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set, training_set = create_sets(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print first_20percent_of_u1 == test_set['u1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_100k_test_set has  1105 keys\n",
      "movie_100k_training_set has  1108  keys\n"
     ]
    }
   ],
   "source": [
    "print \"movie_100k_test_set has \", len(test_set.keys()), \"keys\"\n",
    "print \"movie_100k_training_set has \", len(training_set.keys()), \" keys\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-User Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_user_similarity(graph):\n",
    "    users = []\n",
    "    for key in graph:\n",
    "        if 'u' in key:\n",
    "            users.append(key)\n",
    "    similarity = np.zeros((1600+1, 1600+1))\n",
    "    count = np.zeros((1600+1,1600+1))\n",
    "    #similarity = np.zeros((len(users) + 1, len(users) + 1))\n",
    "    #count = np.zeros((len(users) + 1, len(users) + 1))\n",
    "    np.fill_diagonal(similarity, 1)\n",
    "    np.fill_diagonal(count, 1)\n",
    "    for u1 in users:\n",
    "        for u2 in users:\n",
    "            if u1 == u2:\n",
    "                continue\n",
    "            for item in graph[u1]:\n",
    "                if item in graph[u2]:\n",
    "                    count[int(u1[1:]), int(u2[1:])] += 1\n",
    "                    if user_item_map[u1][item] == user_item_map[u2][item]:\n",
    "                        similarity[int(u1[1:]), int(u2[1:])] += 1\n",
    "                    else:\n",
    "                        similarity[int(u1[1:]), int(u2[1:])] -= 1\n",
    "            count[int(u2[1:]), int(u1[1:])] = count[int(u1[1:]), int(u2[1:])]\n",
    "            similarity[int(u2[1:]), int(u1[1:])] = similarity[int(u1[1:]), int(u2[1:])]\n",
    "    return np.divide(similarity, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "user_similarity = find_user_similarity(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_similarity[np.isnan(user_similarity)] = -1\n",
    "min_value = user_similarity.min()\n",
    "max_value = user_similarity.max()\n",
    "user_similarity = (user_similarity - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.168283720239\n",
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print len(np.argwhere(user_similarity[:, :] > 0.5)) * 1.0 / (1601**2)\n",
    "print min_value\n",
    "print max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Item-Item Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def find_item_similarity(graph):\\n    items = []\\n    for key in graph:\\n        if 'i' in key:\\n            items.append(key)\\n    \\n    similarity = np.zeros((len(items), len(items)))\\n    \\n    # How many times the same users have rated a pair of items\\n    count = np.zeros((len(items), len(items)))\\n    \\n    np.fill_diagonal(similarity, 1)\\n    np.fill_diagonal(count, 1)\\n\\n    # Dictionary to map item ID to index in similarity matrix \\n    id_to_index = {} \\n    \\n    for i in range(len(items)): \\n        id_to_index[items[i]] = i\\n    \\n    # Fill similarity/count matrices \\n    for i1 in items: \\n        for i2 in items: \\n            if i1 == i2: \\n                continue \\n            \\n            i1_id = id_to_index[i1] \\n            i2_id = id_to_index[i2]\\n            \\n            for user in graph[i1]: # for i1, list of users who have rated it\\n                if user in graph[i2]: # if that same user has also rated i2  \\n                    count[i1_id, i2_id] += 1\\n                    \\n                    # If this user rated i1 and i2 the same \\n                    if user_item_map[user][i1] == user_item_map[user][i2]: \\n                        similarity[i1_id, i2_id] += 1.0\\n                    else: \\n                        similarity[i1_id, i2_id] -= 1.0\\n            \\n            # Upon completing the for loop, make sure the reverse pair in the \\n            # similarity and count matrices have the same value (e.g. make a,b = b,a)\\n            similarity[i2_id, i1_id] = similarity[i1_id, i2_id]\\n            count[i2_id, i1_id] = count[i1_id, i2_id]\\n    return np.divide(similarity, count)\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input: the same graph of both users and items that was used for find_user_similarity\n",
    "# Output: item-item similarity matrix \n",
    "'''def find_item_similarity(graph):\n",
    "    items = []\n",
    "    for key in graph:\n",
    "        if 'i' in key:\n",
    "            items.append(key)\n",
    "    \n",
    "    similarity = np.zeros((len(items), len(items)))\n",
    "    \n",
    "    # How many times the same users have rated a pair of items\n",
    "    count = np.zeros((len(items), len(items)))\n",
    "    \n",
    "    np.fill_diagonal(similarity, 1)\n",
    "    np.fill_diagonal(count, 1)\n",
    "\n",
    "    # Dictionary to map item ID to index in similarity matrix \n",
    "    id_to_index = {} \n",
    "    \n",
    "    for i in range(len(items)): \n",
    "        id_to_index[items[i]] = i\n",
    "    \n",
    "    # Fill similarity/count matrices \n",
    "    for i1 in items: \n",
    "        for i2 in items: \n",
    "            if i1 == i2: \n",
    "                continue \n",
    "            \n",
    "            i1_id = id_to_index[i1] \n",
    "            i2_id = id_to_index[i2]\n",
    "            \n",
    "            for user in graph[i1]: # for i1, list of users who have rated it\n",
    "                if user in graph[i2]: # if that same user has also rated i2  \n",
    "                    count[i1_id, i2_id] += 1\n",
    "                    \n",
    "                    # If this user rated i1 and i2 the same \n",
    "                    if user_item_map[user][i1] == user_item_map[user][i2]: \n",
    "                        similarity[i1_id, i2_id] += 1.0\n",
    "                    else: \n",
    "                        similarity[i1_id, i2_id] -= 1.0\n",
    "            \n",
    "            # Upon completing the for loop, make sure the reverse pair in the \n",
    "            # similarity and count matrices have the same value (e.g. make a,b = b,a)\n",
    "            similarity[i2_id, i1_id] = similarity[i1_id, i2_id]\n",
    "            count[i2_id, i1_id] = count[i1_id, i2_id]\n",
    "    return np.divide(similarity, count)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_item_similarity(graph):\n",
    "    items = []\n",
    "    for key in graph:\n",
    "        if 'i' in key:\n",
    "            items.append(key)\n",
    "    similarity = np.zeros((2071+1, 2071+1))\n",
    "    count = np.zeros((2071+1,2071+1))\n",
    "    #similarity = np.zeros((len(users) + 1, len(users) + 1))\n",
    "    #count = np.zeros((len(users) + 1, len(users) + 1))\n",
    "    np.fill_diagonal(similarity, 1)\n",
    "    np.fill_diagonal(count, 1)\n",
    "    for i1 in items:\n",
    "        for i2 in items:\n",
    "            if i1 == i2:\n",
    "                continue\n",
    "            for user in graph[i1]:\n",
    "                if user in graph[i2]:\n",
    "                    count[int(i1[1:]), int(i2[1:])] += 1\n",
    "                    if user_item_map[user][i1] == user_item_map[user][i2]:\n",
    "                        similarity[int(i1[1:]), int(i2[1:])] += 1\n",
    "                    else:\n",
    "                        similarity[int(i1[1:]), int(i2[1:])] -= 1\n",
    "            count[int(i2[1:]), int(i1[1:])] = count[int(i1[1:]), int(i2[1:])]\n",
    "            similarity[int(i2[1:]), int(i1[1:])] = similarity[int(i1[1:]), int(i2[1:])]\n",
    "    return np.divide(similarity, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "item_similarity = find_item_similarity(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_similarity[np.isnan(item_similarity)] = -1\n",
    "min_value = item_similarity.min()\n",
    "max_value = item_similarity.max()\n",
    "\n",
    "# Normalize all values in the matrix to be b/w 0 and 1\n",
    "item_similarity = (item_similarity - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2072\n",
      "0.995284618595\n",
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print len(item_similarity)\n",
    "print len(np.argwhere(item_similarity[:, :] == 0)) * 1.0 / (len(item_similarity)**2)\n",
    "print min_value\n",
    "print max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PIS(item_pair):\n",
    "    item1 = item_pair[0]\n",
    "    item2 = item_pair[1]\n",
    "    total = 0\n",
    "    for i in range(0,item_rating_map[item2][0]):\n",
    "        total += np.exp(betaln(item_rating_map[item1][0]+i,item_rating_map[item1][1]+item_rating_map[item2][1]) -\\\n",
    "                        np.log(item_rating_map[item2][1]+i) - \\\n",
    "                        betaln(1+i, item_rating_map[item2][1]) -\\\n",
    "                        betaln(item_rating_map[item1][0],item_rating_map[item1][1])\n",
    "                       )\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PPS(item_pair):\n",
    "    item1 = item_pair[0]\n",
    "    item2 = item_pair[1]\n",
    "    p1 = (item_rating_map[item1][0] + 1) * 1.0 / (item_rating_map[item1][0] + item_rating_map[item1][1] + 1)\n",
    "    p2 = (item_rating_map[item2][0] + 1) * 1.0 / (item_rating_map[item2][0] + item_rating_map[item2][1] + 1)\n",
    "    return p1 * p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PORS(item_pair):\n",
    "    item1 = item_pair[0]\n",
    "    item2 = item_pair[1]\n",
    "    o1 = (item_rating_map[item1][0] + 1) * 1.0 / (item_rating_map[item1][1] + 1)\n",
    "    o2 = (item_rating_map[item2][0] + 1) * 1.0 / (item_rating_map[item2][1] + 1)\n",
    "    return o2 / o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def USS(u1, i1, u2, i2):\n",
    "    reliability = item_rating_map[i2][0] * 1.0 / (item_rating_map[i2][0] + item_rating_map[i2][1])\n",
    "    indicator = 1\n",
    "    if user_item_map[u2][i2] == 0:\n",
    "        indicator = 0\n",
    "    similarity = user_similarity[int(u1[1:]), int(u2[1:])]\n",
    "    return reliability * similarity * indicator + reliability * (1.0 - similarity) * (1 - indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: A pair of user-item \n",
    "# Output: item similarity score \n",
    "def ISS(u1, i1, u2, i2):\n",
    "    reliability = item_rating_map[i2][0] * 1.0 / (item_rating_map[i2][0] + item_rating_map[i2][1])\n",
    "    indicator = 1\n",
    "    if user_item_map[u2][i2] == 0:\n",
    "        indicator = 0\n",
    "    similarity = item_similarity[int(i1[1:]), int(i2[1:])]\n",
    "    return reliability * similarity * indicator + reliability * (1.0 - similarity) * (1 - indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevance = {}\n",
    "maximum = 0\n",
    "\n",
    "for key in training_set:\n",
    "    if 'u' in key:\n",
    "        relevance[key] = len(training_set[key])\n",
    "        if maximum < relevance[key]:\n",
    "            maximum = relevance[key]\n",
    "            \n",
    "for key in relevance:\n",
    "    relevance[key] = relevance[key] * 1.0 / maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(graph, target_user):\n",
    "    PIS_values = defaultdict(float)\n",
    "    PPS_values = defaultdict(float)\n",
    "    PORS_values = defaultdict(float)\n",
    "    \n",
    "    score_PIS = defaultdict(float)\n",
    "    score_PPS = defaultdict(float)\n",
    "    score_PORS = defaultdict(float)\n",
    "    \n",
    "    score_PIS_USS = defaultdict(float)\n",
    "    score_PPS_USS = defaultdict(float)\n",
    "    score_PORS_USS = defaultdict(float)\n",
    "    \n",
    "    score_PIS_ISS = defaultdict(float)\n",
    "    score_PPS_ISS = defaultdict(float)\n",
    "    score_PORS_ISS = defaultdict(float)\n",
    "    \n",
    "    score_PIS_USS_ISS = defaultdict(float)\n",
    "    score_PPS_USS_ISS = defaultdict(float)\n",
    "    score_PORS_USS_ISS = defaultdict(float)\n",
    "    \n",
    "    score_rel_PIS = defaultdict(float)\n",
    "    score_rel_PPS = defaultdict(float)\n",
    "    score_rel_PORS = defaultdict(float)\n",
    "    \n",
    "    score_rel_PIS_USS = defaultdict(float)\n",
    "    score_rel_PPS_USS = defaultdict(float)\n",
    "    score_rel_PORS_USS = defaultdict(float)\n",
    "    \n",
    "    score_rel_PIS_ISS = defaultdict(float)\n",
    "    score_rel_PPS_ISS = defaultdict(float)\n",
    "    score_rel_PORS_ISS = defaultdict(float)\n",
    "    \n",
    "    score_rel_PIS_USS_ISS = defaultdict(float)\n",
    "    score_rel_PPS_USS_ISS = defaultdict(float)\n",
    "    score_rel_PORS_USS_ISS = defaultdict(float)\n",
    "    \n",
    "    for primary_item in graph[target_user]:\n",
    "        for secondary_user in graph[primary_item]:\n",
    "            if secondary_user == target_user:\n",
    "                continue\n",
    "            for secondary_item in graph[secondary_user]:\n",
    "                if secondary_item in graph[target_user]:\n",
    "                    continue\n",
    "                \n",
    "                if (primary_item, secondary_item) not in PIS_values:\n",
    "                    PIS_values[(primary_item, secondary_item)] = PIS((primary_item, secondary_item))\n",
    "                score_PIS[secondary_item] += PIS_values[(primary_item, secondary_item)]\n",
    "                score_PIS_USS[secondary_item] += PIS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])]\n",
    "                score_PIS_ISS[secondary_item] += PIS_values[(primary_item, secondary_item)] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])]\n",
    "                score_PIS_USS_ISS[secondary_item] += PIS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])]\n",
    "                \n",
    "                score_rel_PIS[secondary_item] += PIS_values[(primary_item, secondary_item)] * relevance[secondary_user]\n",
    "                score_rel_PIS_USS[secondary_item] += PIS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * relevance[secondary_user]\n",
    "                score_rel_PIS_ISS[secondary_item] += PIS_values[(primary_item, secondary_item)] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])] * relevance[secondary_user]\n",
    "                score_rel_PIS_USS_ISS[secondary_item] += PIS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])] * relevance[secondary_user]\n",
    "                \n",
    "                \n",
    "                if (primary_item, secondary_item) not in PPS_values:\n",
    "                    PPS_values[(primary_item, secondary_item)] = PPS((primary_item, secondary_item))\n",
    "                score_PPS[secondary_item] += PPS_values[(primary_item, secondary_item)]\n",
    "                score_PPS_USS[secondary_item] += PPS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])]\n",
    "                score_PPS_ISS[secondary_item] += PPS_values[(primary_item, secondary_item)] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])]\n",
    "                score_PPS_USS_ISS[secondary_item] += PPS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])]\n",
    "                \n",
    "                score_rel_PPS[secondary_item] += PPS_values[(primary_item, secondary_item)] * relevance[secondary_user]\n",
    "                score_rel_PPS_USS[secondary_item] += PPS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * relevance[secondary_user]\n",
    "                score_rel_PPS_ISS[secondary_item] += PPS_values[(primary_item, secondary_item)] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])] * relevance[secondary_user]\n",
    "                score_rel_PPS_USS_ISS[secondary_item] += PPS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])] * relevance[secondary_user]\n",
    "                \n",
    "                \n",
    "                if (primary_item, secondary_item) not in PORS_values:\n",
    "                    PORS_values[(primary_item, secondary_item)] = PORS((primary_item, secondary_item))\n",
    "                score_PORS[secondary_item] += PORS_values[(primary_item, secondary_item)]\n",
    "                score_PORS_USS[secondary_item] += PORS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])]\n",
    "                score_PORS_ISS[secondary_item] += PORS_values[(primary_item, secondary_item)] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])]\n",
    "                score_PORS_USS_ISS[secondary_item] += PORS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])]\n",
    "                \n",
    "                score_rel_PORS[secondary_item] += PORS_values[(primary_item, secondary_item)] * relevance[secondary_user]\n",
    "                score_rel_PORS_USS[secondary_item] += PORS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * relevance[secondary_user]\n",
    "                score_rel_PORS_ISS[secondary_item] += PORS_values[(primary_item, secondary_item)] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])] * relevance[secondary_user]\n",
    "                score_rel_PORS_USS_ISS[secondary_item] += PORS_values[(primary_item, secondary_item)] * user_similarity[int(target_user[1:]), int(secondary_user[1:])] * item_similarity[int(primary_item[1:]), int(secondary_item[1:])] * relevance[secondary_user]\n",
    "\n",
    "    return score_PIS, score_PIS_USS, score_PIS_ISS, score_PIS_USS_ISS, score_rel_PIS, score_rel_PIS_USS, score_rel_PIS_ISS, score_rel_PIS_USS_ISS, score_PPS, score_PPS_USS, score_PPS_ISS, score_PPS_USS_ISS, score_rel_PPS, score_rel_PPS_USS, score_rel_PPS_ISS, score_rel_PPS_USS_ISS, score_PORS, score_PORS_USS, score_PORS_ISS, score_PORS_USS_ISS, score_rel_PORS, score_rel_PORS_USS, score_rel_PORS_ISS, score_rel_PORS_USS_ISS \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 3 lists of ranking scores per item for PIS, PPS, PORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Users done\n",
      "2  Users done\n",
      "3  Users done\n",
      "4  Users done\n",
      "5  Users done\n",
      "6  Users done\n",
      "7  Users done\n",
      "8  Users done\n",
      "9  Users done\n",
      "10  Users done\n",
      "11  Users done\n",
      "12  Users done\n",
      "13  Users done\n",
      "14  Users done\n",
      "15  Users done\n",
      "16  Users done\n",
      "17  Users done\n",
      "18  Users done\n",
      "19  Users done\n",
      "20  Users done\n",
      "21  Users done\n",
      "22  Users done\n",
      "23  Users done\n",
      "24  Users done\n",
      "25  Users done\n",
      "26  Users done\n",
      "27  Users done\n",
      "28  Users done\n",
      "29  Users done\n",
      "30  Users done\n",
      "31  Users done\n",
      "32  Users done\n",
      "33  Users done\n",
      "34  Users done\n",
      "35  Users done\n",
      "36  Users done\n",
      "37  Users done\n",
      "38  Users done\n",
      "39  Users done\n",
      "40  Users done\n",
      "41  Users done\n",
      "42  Users done\n",
      "43  Users done\n",
      "44  Users done\n",
      "45  Users done\n",
      "46  Users done\n",
      "47  Users done\n",
      "48  Users done\n",
      "49  Users done\n",
      "50  Users done\n",
      "51  Users done\n",
      "52  Users done\n",
      "53  Users done\n",
      "54  Users done\n",
      "55  Users done\n",
      "56  Users done\n",
      "57  Users done\n",
      "58  Users done\n",
      "59  Users done\n",
      "60  Users done\n",
      "61  Users done\n",
      "62  Users done\n",
      "63  Users done\n",
      "64  Users done\n",
      "65  Users done\n",
      "66  Users done\n",
      "67  Users done\n",
      "68  Users done\n",
      "69  Users done\n",
      "70  Users done\n",
      "71  Users done\n",
      "72  Users done\n",
      "73  Users done\n",
      "74  Users done\n",
      "75  Users done\n",
      "76  Users done\n",
      "77  Users done\n",
      "78  Users done\n",
      "79  Users done\n",
      "80  Users done\n",
      "81  Users done\n",
      "82  Users done\n",
      "83  Users done\n",
      "84  Users done\n",
      "85  Users done\n",
      "86  Users done\n",
      "87  Users done\n",
      "88  Users done\n",
      "89  Users done\n",
      "90  Users done\n",
      "91  Users done\n",
      "92  Users done\n",
      "93  Users done\n",
      "94  Users done\n",
      "95  Users done\n",
      "96  Users done\n",
      "97  Users done\n",
      "98  Users done\n",
      "99  Users done\n",
      "100  Users done\n",
      "101  Users done\n",
      "102  Users done\n",
      "103  Users done\n",
      "104  Users done\n",
      "105  Users done\n",
      "106  Users done\n",
      "107  Users done\n",
      "108  Users done\n",
      "109  Users done\n",
      "110  Users done\n",
      "111  Users done\n",
      "112  Users done\n",
      "113  Users done\n",
      "114  Users done\n",
      "115  Users done\n",
      "116  Users done\n",
      "117  Users done\n",
      "118  Users done\n",
      "119  Users done\n",
      "120  Users done\n",
      "121  Users done\n",
      "122  Users done\n",
      "123  Users done\n",
      "124  Users done\n",
      "125  Users done\n",
      "126  Users done\n",
      "127  Users done\n",
      "128  Users done\n",
      "129  Users done\n",
      "130  Users done\n",
      "131  Users done\n",
      "132  Users done\n",
      "133  Users done\n",
      "134  Users done\n",
      "135  Users done\n",
      "136  Users done\n",
      "137  Users done\n",
      "138  Users done\n",
      "139  Users done\n",
      "140  Users done\n",
      "141  Users done\n",
      "142  Users done\n",
      "143  Users done\n",
      "144  Users done\n",
      "145  Users done\n",
      "146  Users done\n",
      "147  Users done\n",
      "148  Users done\n",
      "149  Users done\n",
      "150  Users done\n",
      "151  Users done\n",
      "152  Users done\n",
      "153  Users done\n",
      "154  Users done\n",
      "155  Users done\n",
      "156  Users done\n",
      "157  Users done\n",
      "158  Users done\n",
      "159  Users done\n",
      "160  Users done\n",
      "161  Users done\n",
      "162  Users done\n",
      "163  Users done\n",
      "164  Users done\n",
      "165  Users done\n",
      "166  Users done\n",
      "167  Users done\n",
      "168  Users done\n",
      "169  Users done\n",
      "170  Users done\n",
      "171  Users done\n",
      "172  Users done\n",
      "173  Users done\n",
      "174  Users done\n",
      "175  Users done\n",
      "176  Users done\n",
      "177  Users done\n",
      "178  Users done\n",
      "179  Users done\n",
      "180  Users done\n",
      "181  Users done\n",
      "182  Users done\n",
      "183  Users done\n",
      "184  Users done\n",
      "185  Users done\n",
      "186  Users done\n",
      "187  Users done\n",
      "188  Users done\n",
      "189  Users done\n",
      "190  Users done\n",
      "191  Users done\n",
      "192  Users done\n",
      "193  Users done\n",
      "194  Users done\n",
      "195  Users done\n",
      "196  Users done\n",
      "197  Users done\n",
      "198  Users done\n",
      "199  Users done\n",
      "200  Users done\n",
      "201  Users done\n",
      "202  Users done\n",
      "203  Users done\n",
      "204  Users done\n",
      "205  Users done\n",
      "206  Users done\n",
      "207  Users done\n",
      "208  Users done\n",
      "209  Users done\n",
      "210  Users done\n",
      "211  Users done\n",
      "212  Users done\n",
      "213  Users done\n",
      "214  Users done\n",
      "215  Users done\n",
      "216  Users done\n",
      "217  Users done\n",
      "218  Users done\n",
      "219  Users done\n",
      "220  Users done\n",
      "221  Users done\n",
      "222  Users done\n",
      "223  Users done\n",
      "224  Users done\n",
      "225  Users done\n",
      "226  Users done\n",
      "227  Users done\n",
      "228  Users done\n",
      "229  Users done\n",
      "230  Users done\n",
      "231  Users done\n",
      "232  Users done\n",
      "233  Users done\n",
      "234  Users done\n",
      "235  Users done\n",
      "236  Users done\n",
      "237  Users done\n",
      "238  Users done\n",
      "239  Users done\n",
      "240  Users done\n",
      "241  Users done\n",
      "242  Users done\n",
      "243  Users done\n",
      "244  Users done\n",
      "245  Users done\n",
      "246  Users done\n",
      "247  Users done\n",
      "248  Users done\n",
      "249  Users done\n",
      "250  Users done\n",
      "251  Users done\n",
      "252  Users done\n",
      "253  Users done\n",
      "254  Users done\n",
      "255  Users done\n",
      "256  Users done\n",
      "257  Users done\n",
      "258  Users done\n",
      "259  Users done\n",
      "260  Users done\n",
      "261  Users done\n",
      "262  Users done\n",
      "263  Users done\n",
      "264  Users done\n",
      "265  Users done\n",
      "266  Users done\n",
      "267  Users done\n",
      "268  Users done\n",
      "269  Users done\n",
      "270  Users done\n",
      "271  Users done\n",
      "272  Users done\n",
      "273  Users done\n",
      "274  Users done\n",
      "275  Users done\n",
      "276  Users done\n",
      "277  Users done\n",
      "278  Users done\n",
      "279  Users done\n",
      "280  Users done\n",
      "281  Users done\n",
      "282  Users done\n",
      "283  Users done\n",
      "284  Users done\n",
      "285  Users done\n",
      "286  Users done\n",
      "287  Users done\n",
      "288  Users done\n",
      "289  Users done\n",
      "290  Users done\n",
      "291  Users done\n",
      "292  Users done\n",
      "293  Users done\n",
      "294  Users done\n",
      "295  Users done\n",
      "296  Users done\n",
      "297  Users done\n",
      "298  Users done\n",
      "299  Users done\n",
      "300  Users done\n",
      "301  Users done\n",
      "302  Users done\n",
      "303  Users done\n",
      "304  Users done\n",
      "305  Users done\n",
      "306  Users done\n",
      "307  Users done\n",
      "308  Users done\n",
      "309  Users done\n",
      "310  Users done\n",
      "311  Users done\n",
      "312  Users done\n",
      "313  Users done\n",
      "314  Users done\n",
      "315  Users done\n",
      "316  Users done\n",
      "317  Users done\n",
      "318  Users done\n",
      "319  Users done\n",
      "320  Users done\n",
      "321  Users done\n",
      "322  Users done\n",
      "323  Users done\n",
      "324  Users done\n",
      "325  Users done\n",
      "326  Users done\n",
      "327  Users done\n",
      "328  Users done\n",
      "329  Users done\n",
      "330  Users done\n",
      "331  Users done\n",
      "332  Users done\n",
      "333  Users done\n",
      "334  Users done\n",
      "335  Users done\n",
      "336  Users done\n",
      "337  Users done\n",
      "338  Users done\n",
      "339  Users done\n",
      "340  Users done\n",
      "341  Users done\n",
      "342  Users done\n",
      "343  Users done\n",
      "344  Users done\n",
      "345  Users done\n",
      "346  Users done\n",
      "347  Users done\n",
      "348  Users done\n",
      "349  Users done\n",
      "350  Users done\n",
      "351  Users done\n",
      "352  Users done\n",
      "353  Users done\n",
      "354  Users done\n",
      "355  Users done\n",
      "356  Users done\n",
      "357  Users done\n",
      "358  Users done\n",
      "359  Users done\n",
      "360  Users done\n",
      "361  Users done\n",
      "362  Users done\n",
      "363  Users done\n",
      "364  Users done\n",
      "365  Users done\n",
      "366  Users done\n",
      "367  Users done\n",
      "368  Users done\n",
      "369  Users done\n",
      "370  Users done\n",
      "371  Users done\n",
      "372  Users done\n",
      "373  Users done\n",
      "374  Users done\n",
      "375  Users done\n",
      "376  Users done\n",
      "377  Users done\n",
      "378  Users done\n",
      "379  Users done\n",
      "380  Users done\n",
      "381  Users done\n",
      "382  Users done\n",
      "383  Users done\n",
      "384  Users done\n",
      "385  Users done\n",
      "386  Users done\n",
      "387  Users done\n",
      "388  Users done\n",
      "389  Users done\n",
      "390  Users done\n",
      "391  Users done\n",
      "392  Users done\n",
      "393  Users done\n",
      "394  Users done\n",
      "395  Users done\n",
      "396  Users done\n",
      "397  Users done\n",
      "398  Users done\n",
      "399  Users done\n",
      "400  Users done\n",
      "401  Users done\n",
      "402  Users done\n",
      "403  Users done\n",
      "404  Users done\n",
      "405  Users done\n",
      "406  Users done\n",
      "407  Users done\n",
      "408  Users done\n",
      "409  Users done\n",
      "410  Users done\n",
      "411  Users done\n",
      "412  Users done\n",
      "413  Users done\n",
      "414  Users done\n",
      "415  Users done\n",
      "416  Users done\n",
      "417  Users done\n",
      "418  Users done\n",
      "419  Users done\n",
      "420  Users done\n",
      "421  Users done\n",
      "422  Users done\n",
      "423  Users done\n",
      "424  Users done\n",
      "425  Users done\n",
      "426  Users done\n",
      "427  Users done\n",
      "428  Users done\n",
      "429  Users done\n",
      "430  Users done\n",
      "431  Users done\n",
      "432  Users done\n",
      "433  Users done\n",
      "434  Users done\n",
      "435  Users done\n",
      "436  Users done\n",
      "437  Users done\n",
      "438  Users done\n",
      "439  Users done\n",
      "440  Users done\n",
      "441  Users done\n",
      "442  Users done\n",
      "443  Users done\n",
      "444  Users done\n",
      "445  Users done\n",
      "446  Users done\n",
      "447  Users done\n",
      "448  Users done\n",
      "449  Users done\n",
      "450  Users done\n",
      "451  Users done\n",
      "452  Users done\n",
      "453  Users done\n",
      "454  Users done\n",
      "455  Users done\n",
      "456  Users done\n",
      "457  Users done\n",
      "458  Users done\n",
      "459  Users done\n",
      "460  Users done\n",
      "461  Users done\n",
      "462  Users done\n",
      "463  Users done\n",
      "464  Users done\n",
      "465  Users done\n",
      "466  Users done\n",
      "467  Users done\n",
      "468  Users done\n",
      "469  Users done\n",
      "470  Users done\n",
      "471  Users done\n",
      "472  Users done\n",
      "473  Users done\n",
      "474  Users done\n",
      "475  Users done\n",
      "476  Users done\n",
      "477  Users done\n",
      "478  Users done\n",
      "479  Users done\n",
      "480  Users done\n",
      "481  Users done\n",
      "482  Users done\n",
      "483  Users done\n",
      "484  Users done\n",
      "485  Users done\n",
      "486  Users done\n",
      "487  Users done\n",
      "488  Users done\n",
      "489  Users done\n",
      "490  Users done\n",
      "491  Users done\n",
      "492  Users done\n",
      "493  Users done\n",
      "494  Users done\n",
      "495  Users done\n",
      "496  Users done\n",
      "497  Users done\n",
      "498  Users done\n",
      "499  Users done\n",
      "500  Users done\n",
      "501  Users done\n",
      "502  Users done\n",
      "503  Users done\n",
      "504  Users done\n",
      "505  Users done\n",
      "506  Users done\n",
      "507  Users done\n",
      "508  Users done\n",
      "509  Users done\n",
      "510  Users done\n",
      "511  Users done\n",
      "512  Users done\n",
      "513  Users done\n",
      "514  Users done\n",
      "515  Users done\n",
      "516  Users done\n",
      "517  Users done\n",
      "518  Users done\n",
      "519  Users done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520  Users done\n",
      "521  Users done\n",
      "522  Users done\n",
      "523  Users done\n",
      "524  Users done\n",
      "525  Users done\n",
      "526  Users done\n",
      "527  Users done\n",
      "528  Users done\n",
      "529  Users done\n",
      "530  Users done\n",
      "531  Users done\n",
      "532  Users done\n",
      "533  Users done\n",
      "534  Users done\n",
      "535  Users done\n",
      "536  Users done\n",
      "537  Users done\n",
      "538  Users done\n",
      "539  Users done\n",
      "540  Users done\n",
      "541  Users done\n",
      "542  Users done\n",
      "543  Users done\n",
      "544  Users done\n",
      "545  Users done\n",
      "546  Users done\n",
      "547  Users done\n",
      "548  Users done\n",
      "549  Users done\n",
      "550  Users done\n",
      "551  Users done\n",
      "552  Users done\n",
      "553  Users done\n",
      "554  Users done\n",
      "555  Users done\n",
      "556  Users done\n",
      "557  Users done\n",
      "558  Users done\n",
      "559  Users done\n",
      "560  Users done\n",
      "561  Users done\n",
      "562  Users done\n",
      "563  Users done\n",
      "564  Users done\n",
      "565  Users done\n",
      "566  Users done\n",
      "567  Users done\n",
      "568  Users done\n",
      "569  Users done\n",
      "570  Users done\n",
      "571  Users done\n",
      "572  Users done\n",
      "573  Users done\n",
      "574  Users done\n",
      "575  Users done\n",
      "576  Users done\n",
      "577  Users done\n",
      "578  Users done\n",
      "579  Users done\n",
      "580  Users done\n",
      "581  Users done\n",
      "582  Users done\n",
      "583  Users done\n",
      "584  Users done\n",
      "585  Users done\n",
      "586  Users done\n",
      "587  Users done\n",
      "588  Users done\n",
      "589  Users done\n",
      "590  Users done\n",
      "591  Users done\n",
      "592  Users done\n",
      "593  Users done\n",
      "594  Users done\n",
      "595  Users done\n",
      "596  Users done\n",
      "597  Users done\n",
      "598  Users done\n",
      "599  Users done\n",
      "600  Users done\n",
      "601  Users done\n",
      "602  Users done\n",
      "603  Users done\n",
      "604  Users done\n",
      "605  Users done\n",
      "606  Users done\n",
      "607  Users done\n",
      "608  Users done\n",
      "609  Users done\n",
      "610  Users done\n",
      "611  Users done\n",
      "612  Users done\n",
      "613  Users done\n",
      "614  Users done\n",
      "615  Users done\n",
      "616  Users done\n",
      "617  Users done\n",
      "618  Users done\n",
      "619  Users done\n",
      "620  Users done\n",
      "621  Users done\n",
      "622  Users done\n",
      "623  Users done\n",
      "624  Users done\n",
      "625  Users done\n",
      "626  Users done\n",
      "627  Users done\n",
      "628  Users done\n",
      "629  Users done\n",
      "630  Users done\n",
      "631  Users done\n",
      "632  Users done\n",
      "633  Users done\n",
      "634  Users done\n",
      "635  Users done\n",
      "636  Users done\n",
      "637  Users done\n",
      "638  Users done\n",
      "639  Users done\n",
      "640  Users done\n",
      "641  Users done\n",
      "642  Users done\n",
      "643  Users done\n",
      "644  Users done\n",
      "645  Users done\n",
      "646  Users done\n",
      "647  Users done\n",
      "648  Users done\n",
      "649  Users done\n",
      "650  Users done\n",
      "651  Users done\n",
      "652  Users done\n",
      "653  Users done\n",
      "654  Users done\n",
      "655  Users done\n",
      "656  Users done\n",
      "657  Users done\n",
      "658  Users done\n",
      "659  Users done\n",
      "660  Users done\n",
      "661  Users done\n",
      "662  Users done\n",
      "663  Users done\n",
      "664  Users done\n",
      "665  Users done\n",
      "666  Users done\n",
      "667  Users done\n",
      "668  Users done\n",
      "669  Users done\n",
      "670  Users done\n",
      "671  Users done\n",
      "672  Users done\n",
      "673  Users done\n",
      "674  Users done\n",
      "675  Users done\n",
      "676  Users done\n",
      "677  Users done\n",
      "678  Users done\n",
      "679  Users done\n",
      "680  Users done\n",
      "681  Users done\n",
      "682  Users done\n",
      "683  Users done\n",
      "684  Users done\n",
      "685  Users done\n",
      "686  Users done\n",
      "687  Users done\n",
      "688  Users done\n",
      "689  Users done\n",
      "690  Users done\n",
      "691  Users done\n",
      "692  Users done\n",
      "693  Users done\n",
      "694  Users done\n",
      "695  Users done\n",
      "696  Users done\n",
      "697  Users done\n",
      "698  Users done\n",
      "699  Users done\n",
      "700  Users done\n",
      "701  Users done\n",
      "702  Users done\n",
      "703  Users done\n",
      "704  Users done\n",
      "705  Users done\n",
      "706  Users done\n",
      "707  Users done\n",
      "708  Users done\n",
      "709  Users done\n",
      "710  Users done\n",
      "711  Users done\n",
      "712  Users done\n",
      "713  Users done\n",
      "714  Users done\n",
      "715  Users done\n",
      "716  Users done\n",
      "717  Users done\n",
      "718  Users done\n",
      "719  Users done\n",
      "720  Users done\n",
      "721  Users done\n",
      "722  Users done\n",
      "723  Users done\n",
      "724  Users done\n",
      "725  Users done\n",
      "726  Users done\n",
      "727  Users done\n",
      "728  Users done\n",
      "729  Users done\n",
      "730  Users done\n",
      "731  Users done\n",
      "732  Users done\n",
      "733  Users done\n",
      "734  Users done\n",
      "735  Users done\n",
      "736  Users done\n",
      "737  Users done\n",
      "738  Users done\n",
      "739  Users done\n",
      "740  Users done\n",
      "741  Users done\n",
      "742  Users done\n",
      "743  Users done\n",
      "744  Users done\n",
      "745  Users done\n",
      "746  Users done\n",
      "747  Users done\n",
      "748  Users done\n",
      "749  Users done\n",
      "750  Users done\n",
      "751  Users done\n",
      "752  Users done\n",
      "753  Users done\n",
      "754  Users done\n",
      "755  Users done\n",
      "756  Users done\n",
      "757  Users done\n",
      "758  Users done\n",
      "759  Users done\n",
      "760  Users done\n",
      "761  Users done\n",
      "762  Users done\n",
      "763  Users done\n",
      "764  Users done\n",
      "765  Users done\n",
      "766  Users done\n",
      "767  Users done\n",
      "768  Users done\n",
      "769  Users done\n",
      "770  Users done\n",
      "771  Users done\n",
      "772  Users done\n",
      "773  Users done\n",
      "774  Users done\n",
      "775  Users done\n",
      "776  Users done\n",
      "777  Users done\n",
      "778  Users done\n",
      "779  Users done\n",
      "780  Users done\n",
      "781  Users done\n",
      "782  Users done\n",
      "783  Users done\n",
      "784  Users done\n",
      "785  Users done\n",
      "786  Users done\n",
      "787  Users done\n",
      "788  Users done\n",
      "789  Users done\n",
      "790  Users done\n",
      "791  Users done\n",
      "792  Users done\n",
      "793  Users done\n",
      "794  Users done\n",
      "795  Users done\n",
      "796  Users done\n",
      "797  Users done\n",
      "798  Users done\n",
      "799  Users done\n",
      "800  Users done\n",
      "801  Users done\n",
      "802  Users done\n",
      "803  Users done\n",
      "804  Users done\n",
      "805  Users done\n",
      "806  Users done\n",
      "807  Users done\n",
      "808  Users done\n",
      "809  Users done\n",
      "810  Users done\n",
      "811  Users done\n",
      "812  Users done\n",
      "813  Users done\n",
      "814  Users done\n",
      "815  Users done\n",
      "816  Users done\n",
      "817  Users done\n",
      "818  Users done\n",
      "819  Users done\n",
      "820  Users done\n",
      "821  Users done\n",
      "822  Users done\n",
      "823  Users done\n",
      "824  Users done\n",
      "825  Users done\n",
      "826  Users done\n",
      "827  Users done\n",
      "828  Users done\n",
      "829  Users done\n",
      "830  Users done\n",
      "831  Users done\n",
      "832  Users done\n",
      "833  Users done\n",
      "834  Users done\n",
      "835  Users done\n",
      "836  Users done\n",
      "837  Users done\n",
      "838  Users done\n",
      "839  Users done\n",
      "840  Users done\n",
      "841  Users done\n",
      "842  Users done\n",
      "843  Users done\n",
      "844  Users done\n",
      "845  Users done\n",
      "846  Users done\n",
      "847  Users done\n",
      "848  Users done\n",
      "849  Users done\n",
      "850  Users done\n",
      "851  Users done\n",
      "852  Users done\n",
      "853  Users done\n",
      "854  Users done\n",
      "855  Users done\n",
      "856  Users done\n",
      "857  Users done\n",
      "858  Users done\n",
      "859  Users done\n",
      "860  Users done\n",
      "861  Users done\n",
      "862  Users done\n",
      "863  Users done\n",
      "864  Users done\n",
      "865  Users done\n",
      "866  Users done\n",
      "867  Users done\n",
      "868  Users done\n",
      "869  Users done\n",
      "870  Users done\n",
      "871  Users done\n",
      "872  Users done\n",
      "873  Users done\n",
      "874  Users done\n",
      "875  Users done\n",
      "876  Users done\n",
      "877  Users done\n",
      "878  Users done\n",
      "879  Users done\n",
      "880  Users done\n",
      "881  Users done\n",
      "882  Users done\n",
      "883  Users done\n",
      "884  Users done\n",
      "885  Users done\n",
      "886  Users done\n",
      "887  Users done\n",
      "888  Users done\n",
      "889  Users done\n",
      "890  Users done\n",
      "891  Users done\n",
      "892  Users done\n",
      "893  Users done\n",
      "894  Users done\n",
      "895  Users done\n",
      "896  Users done\n",
      "897  Users done\n",
      "898  Users done\n",
      "899  Users done\n",
      "900  Users done\n",
      "901  Users done\n",
      "902  Users done\n",
      "903  Users done\n",
      "904  Users done\n",
      "905  Users done\n",
      "906  Users done\n",
      "907  Users done\n",
      "908  Users done\n",
      "909  Users done\n",
      "910  Users done\n",
      "911  Users done\n",
      "912  Users done\n",
      "913  Users done\n",
      "914  Users done\n",
      "915  Users done\n",
      "916  Users done\n",
      "917  Users done\n",
      "918  Users done\n",
      "919  Users done\n",
      "920  Users done\n",
      "921  Users done\n",
      "922  Users done\n",
      "923  Users done\n",
      "924  Users done\n",
      "925  Users done\n",
      "926  Users done\n",
      "927  Users done\n",
      "928  Users done\n",
      "929  Users done\n",
      "930  Users done\n",
      "931  Users done\n",
      "932  Users done\n",
      "933  Users done\n",
      "934  Users done\n",
      "935  Users done\n",
      "936  Users done\n",
      "937  Users done\n",
      "938  Users done\n",
      "939  Users done\n",
      "940  Users done\n",
      "941  Users done\n",
      "942  Users done\n",
      "943  Users done\n",
      "944  Users done\n",
      "945  Users done\n",
      "946  Users done\n",
      "947  Users done\n",
      "948  Users done\n",
      "949  Users done\n"
     ]
    }
   ],
   "source": [
    "# ******Caution: the code in this box will need to run overnight due to the size of the test_set****** \n",
    "\n",
    "# Input:  One of the score dictionaries generated by calling rank(graph, key) \n",
    "# Output: A ranking list for each user, where the most highly recommended items come first (no need to store\n",
    "#         the score that was associated with that item)\n",
    "def generate_ranking_list(score_list): \n",
    "    ranking_list = {} \n",
    "\n",
    "    for user in score_list: \n",
    "        # Sort the score list for that user \n",
    "        sorted_score = sorted(score_list[user].items(), key=operator.itemgetter(1), reverse=True)\n",
    "        ranking_list[user]= [item[0] for item in sorted_score]\n",
    "\n",
    "    return ranking_list \n",
    "\n",
    "# For each of the 3 methods, generate scores as a dictionary where \n",
    "# Key = User, Value = List of (item,score) pairs \n",
    "score_PIS = {} \n",
    "score_PPS = {} \n",
    "score_PORS = {}\n",
    "score_PIS_USS = {}\n",
    "score_PPS_USS = {}\n",
    "score_PORS_USS = {}\n",
    "score_PIS_ISS = {}\n",
    "score_PPS_ISS = {}\n",
    "score_PORS_ISS = {}\n",
    "score_PIS_USS_ISS = {}\n",
    "score_PPS_USS_ISS = {}\n",
    "score_PORS_USS_ISS = {}\n",
    "score_rel_PIS = {}\n",
    "score_rel_PPS = {}\n",
    "score_rel_PORS = {}\n",
    "score_rel_PIS_USS = {}\n",
    "score_rel_PPS_USS = {}\n",
    "score_rel_PORS_USS = {}\n",
    "score_rel_PIS_ISS = {}\n",
    "score_rel_PPS_ISS = {}\n",
    "score_rel_PORS_ISS = {}\n",
    "score_rel_PIS_USS_ISS = {}\n",
    "score_rel_PPS_USS_ISS = {}\n",
    "score_rel_PORS_USS_ISS = {}\n",
    "\n",
    "count = 0\n",
    "for user in training_set:\n",
    "    if user[0] == 'u':\n",
    "        score_PIS[user], score_PIS_USS[user], score_PIS_ISS[user], score_PIS_USS_ISS[user], score_rel_PIS[user], score_rel_PIS_USS[user], score_rel_PIS_ISS[user], score_rel_PIS_USS_ISS[user], score_PPS[user], score_PPS_USS[user], score_PPS_ISS[user], score_PPS_USS_ISS[user], score_rel_PPS[user], score_rel_PPS_USS[user], score_rel_PPS_ISS[user], score_rel_PPS_USS_ISS[user], score_PORS[user], score_PORS_USS[user], score_PORS_ISS[user], score_PORS_USS_ISS[user], score_rel_PORS[user], score_rel_PORS_USS[user], score_rel_PORS_ISS[user], score_rel_PORS_USS_ISS[user] = score(training_set, user)\n",
    "        count += 1\n",
    "        if count%1==0:\n",
    "            print count,' Users done'\n",
    "        \n",
    "# Generate 3 lists of ranking scores per item for PIS, PPS, PORS\n",
    "ranking_PIS = generate_ranking_list(score_PIS)\n",
    "ranking_PPS = generate_ranking_list(score_PPS)\n",
    "ranking_PORS = generate_ranking_list(score_PORS)\n",
    "ranking_PIS_USS = generate_ranking_list(score_PIS_USS)\n",
    "ranking_PPS_USS = generate_ranking_list(score_PPS_USS)\n",
    "ranking_PORS_USS = generate_ranking_list(score_PORS_USS)\n",
    "ranking_PIS_ISS = generate_ranking_list(score_PIS_ISS)\n",
    "ranking_PPS_ISS = generate_ranking_list(score_PPS_ISS)\n",
    "ranking_PORS_ISS = generate_ranking_list(score_PORS_ISS)\n",
    "ranking_PIS_USS_ISS = generate_ranking_list(score_PIS_USS_ISS)\n",
    "ranking_PPS_USS_ISS = generate_ranking_list(score_PPS_USS_ISS)\n",
    "ranking_PORS_USS_ISS = generate_ranking_list(score_PORS_USS_ISS)\n",
    "\n",
    "ranking_rel_PIS = generate_ranking_list(score_rel_PIS)\n",
    "ranking_rel_PPS = generate_ranking_list(score_rel_PPS)\n",
    "ranking_rel_PORS = generate_ranking_list(score_rel_PORS)\n",
    "ranking_rel_PIS_USS = generate_ranking_list(score_rel_PIS_USS)\n",
    "ranking_rel_PPS_USS = generate_ranking_list(score_rel_PPS_USS)\n",
    "ranking_rel_PORS_USS = generate_ranking_list(score_rel_PORS_USS)\n",
    "ranking_rel_PIS_ISS = generate_ranking_list(score_rel_PIS_ISS)\n",
    "ranking_rel_PPS_ISS = generate_ranking_list(score_rel_PPS_ISS)\n",
    "ranking_rel_PORS_ISS = generate_ranking_list(score_rel_PORS_ISS)\n",
    "ranking_rel_PIS_USS_ISS = generate_ranking_list(score_rel_PIS_USS_ISS)\n",
    "ranking_rel_PPS_USS_ISS = generate_ranking_list(score_rel_PPS_USS_ISS)\n",
    "ranking_rel_PORS_USS_ISS = generate_ranking_list(score_rel_PORS_USS_ISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lp_ln(predictions):\n",
    "    lp = defaultdict(list)\n",
    "    ln = defaultdict(list)\n",
    "    for user in predictions:\n",
    "        for item in test_set[user]:\n",
    "            if user_item_map[user][item] == 1:\n",
    "                lp[user].append(item)\n",
    "            else:\n",
    "                ln[user].append(item)\n",
    "    return lp,ln\n",
    "\n",
    "def p_at_k(k, predictions, lp):\n",
    "    patk = 0.0\n",
    "    for user in predictions:\n",
    "        correct = 0\n",
    "        for i in range(k):\n",
    "            if predictions[user][i] in lp[user]:\n",
    "                correct+=1\n",
    "        patk += 1.0*correct/k\n",
    "    return patk/len(predictions.keys())\n",
    "\n",
    "def MAP(predictions, lp):\n",
    "    MAP = 0.0\n",
    "    for user in predictions:\n",
    "        umap = 0.0\n",
    "        correct = 0\n",
    "        for i in range(len(predictions[user])):\n",
    "            if predictions[user][i] in lp[user]:\n",
    "                correct += 1\n",
    "                umap += 1.0*correct/(i+1)\n",
    "        MAP += umap/max(1,len(lp[user]))\n",
    "    return MAP/len(predictions.keys())\n",
    "\n",
    "def MRR(predictions, lp):\n",
    "    MRR = 0.0\n",
    "    for user in predictions:\n",
    "        for i in range(len(predictions[user])):\n",
    "            if predictions[user][i] in lp[user]:\n",
    "                MRR += 1.0/(i+1)\n",
    "                break\n",
    "    return MRR/len(predictions.keys())\n",
    "\n",
    "def rel(item, u_lp, u_ln):\n",
    "    if item in u_lp:\n",
    "        return 2\n",
    "    elif item in u_ln:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def dcg_at_n(n, u_pred, u_lp, u_ln):\n",
    "    u_dcg = 0.0\n",
    "    for i in range(0,n):\n",
    "        if i==0:\n",
    "            u_dcg += rel(u_pred[i], u_lp, u_ln)\n",
    "        else:\n",
    "            u_dcg += rel(u_pred[i], u_lp, u_ln)/np.log2(i+1)\n",
    "    return u_dcg\n",
    "    \n",
    "def idcg_at_n(n, u_pred, u_lp, u_ln):\n",
    "    rel_scores = []\n",
    "    for item in u_pred:\n",
    "        rel_scores.append(rel(item, u_lp, u_ln))\n",
    "    rel_scores = sorted(rel_scores, reverse=True)\n",
    "    u_idcg = rel_scores[0]\n",
    "    for i in range(1,n):\n",
    "        u_idcg += rel_scores[i]/np.log2(i+1)\n",
    "    return u_idcg\n",
    "        \n",
    "def ndcg_at_n(n, predictions, lp, ln):\n",
    "    ndcg = 0.0\n",
    "    for user in predictions:\n",
    "        u_dcg = dcg_at_n(n, predictions[user], lp[user], ln[user])\n",
    "        u_idcg = idcg_at_n(n, predictions[user], lp[user], ln[user])\n",
    "        ndcg += u_dcg/u_idcg\n",
    "    return ndcg/len(predictions.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+----------------+----------------+----------------+----------------+----------------+\n",
      "|      Method      |      MAP       |      MRR       |      P@5       |      P@10      |     NDCG@5     |    NDCG@10     |\n",
      "+------------------+----------------+----------------+----------------+----------------+----------------+----------------+\n",
      "|       PIS        | 0.404211430766 | 0.527347874325 | 0.290410958904 | 0.247945205479 | 0.460105823513 | 0.534375075739 |\n",
      "|     PIS_USS      | 0.403320698026 | 0.525100140349 | 0.29062170706  | 0.247629083246 | 0.460091633282 | 0.533897797045 |\n",
      "|     PIS_ISS      | 0.407893143826 | 0.532247995006 | 0.291253951528 | 0.249315068493 | 0.463279865374 | 0.538844301636 |\n",
      "|   PIS_USS_ISS    | 0.408255995475 | 0.532318708387 | 0.291043203372 | 0.249315068493 | 0.463526784114 | 0.539168312156 |\n",
      "|     rel_PIS      | 0.395700690958 | 0.519179718936 | 0.279873551106 | 0.246575342466 | 0.443432622866 | 0.524255942508 |\n",
      "|   rel_PIS_USS    | 0.394992115228 | 0.518549918052 | 0.281348788198 | 0.246786090622 | 0.444003930298 | 0.524173206741 |\n",
      "|   rel_PIS_ISS    | 0.39978108647  | 0.523232004031 | 0.284088514225 | 0.248261327713 | 0.450310194089 | 0.529819935752 |\n",
      "| rel_PIS_USS_ISS  | 0.400016941109 | 0.523972591496 | 0.285142255005 | 0.248577449947 | 0.449082441571 | 0.528707993838 |\n",
      "|       PPS        | 0.424658123465 | 0.541017365252 | 0.308746048472 | 0.257112750263 | 0.497166935815 | 0.572255620015 |\n",
      "|     PPS_USS      | 0.427980688517 | 0.545116843552 | 0.308535300316 | 0.257428872497 | 0.49836239737  | 0.574165133731 |\n",
      "|     PPS_ISS      | 0.425765945389 | 0.540973833747 | 0.30832455216  | 0.257007376185 | 0.497597045417 | 0.571994108164 |\n",
      "|   PPS_USS_ISS    | 0.428425963282 | 0.543122744752 | 0.31043203372  | 0.257428872497 | 0.499436994654 | 0.573844261458 |\n",
      "|     rel_PPS      | 0.414376184251 | 0.53122133439  | 0.297997892518 | 0.254583772392 | 0.476012228864 | 0.555235418281 |\n",
      "|   rel_PPS_USS    | 0.416346244599 | 0.535043844784 | 0.29841938883  | 0.254583772392 | 0.476964373419 | 0.555374179383 |\n",
      "|   rel_PPS_ISS    | 0.414483435252 | 0.531471115972 | 0.303688092729 | 0.253846153846 | 0.48040200188  | 0.553503568913 |\n",
      "| rel_PPS_USS_ISS  | 0.415159191511 | 0.532553914097 | 0.301369863014 | 0.254373024236 | 0.47711468341  | 0.552787283259 |\n",
      "|       PORS       | 0.41208525682  | 0.531548466159 | 0.29841938883  | 0.253424657534 | 0.472834784723 | 0.548683994769 |\n",
      "|     PORS_USS     | 0.412377463106 | 0.531983406556 | 0.299051633298 | 0.253003161222 | 0.472918409467 | 0.548416498602 |\n",
      "|     PORS_ISS     | 0.414870014271 | 0.533805157875 | 0.301369863014 | 0.253951527924 | 0.47866331643  | 0.552887286265 |\n",
      "|   PORS_USS_ISS   | 0.416262762285 | 0.535884971752 | 0.303688092729 | 0.253740779768 | 0.47953924208  | 0.55248779078  |\n",
      "|     rel_PORS     | 0.401163631277 | 0.518517387347 | 0.28282402529  | 0.251949420443 | 0.445905686742 | 0.532635968696 |\n",
      "|   rel_PORS_USS   | 0.400269446636 | 0.517376708935 | 0.282613277134 | 0.251844046365 | 0.445402722604 | 0.531947503847 |\n",
      "|   rel_PORS_ISS   | 0.402307986635 | 0.518493839974 | 0.287038988409 | 0.252476290832 | 0.452175725376 | 0.535506404913 |\n",
      "| rel_PORS_USS_ISS | 0.402592929971 | 0.519213933924 | 0.288514225501 | 0.252265542677 | 0.452856509144 | 0.535573049661 |\n",
      "+------------------+----------------+----------------+----------------+----------------+----------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "t = PrettyTable(['Method', 'MAP', 'MRR', 'P@5', 'P@10', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "predictions = ranking_PIS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PIS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PIS_USS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PIS_USS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PIS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PIS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PIS_USS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PIS_USS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PIS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PIS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PIS_USS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PIS_USS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PIS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PIS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PIS_USS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PIS_USS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "#\n",
    "\n",
    "predictions = ranking_PPS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PPS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PPS_USS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PPS_USS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PPS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PPS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PPS_USS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PPS_USS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PPS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PPS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PPS_USS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PPS_USS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PPS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PPS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PPS_USS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PPS_USS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "#\n",
    "\n",
    "predictions = ranking_PORS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PORS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PORS_USS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PORS_USS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PORS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PORS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_PORS_USS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['PORS_USS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PORS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PORS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PORS_USS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PORS_USS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PORS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PORS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "predictions = ranking_rel_PORS_USS_ISS\n",
    "lp, ln = get_lp_ln(predictions)\n",
    "t.add_row(['rel_PORS_USS_ISS', MAP(predictions, lp), MRR(predictions, lp), p_at_k(5, predictions, lp), p_at_k(10, predictions, lp), ndcg_at_n(5, predictions, lp, ln), ndcg_at_n(10, predictions, lp, ln)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('training_set.pkl', 'w')\n",
    "pickle.dump(training_set, f)\n",
    "f.close()\n",
    "f = open('test_set.pkl', 'w')\n",
    "pickle.dump(test_set, f)\n",
    "f.close()\n",
    "\n",
    "f = open('PIS.pkl', 'w')\n",
    "pickle.dump(ranking_PIS, f)\n",
    "f.close()\n",
    "f = open('PIS_USS.pkl', 'w')\n",
    "pickle.dump(ranking_PIS_USS, f)\n",
    "f.close()\n",
    "f = open('PIS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_PIS_ISS, f)\n",
    "f.close()\n",
    "f = open('PIS_USS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_PIS_USS_ISS, f)\n",
    "f.close()\n",
    "f = open('rel_PIS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PIS, f)\n",
    "f.close()\n",
    "f = open('rel_PIS_USS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PIS_USS, f)\n",
    "f.close()\n",
    "f = open('rel_PIS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PIS_ISS, f)\n",
    "f.close()\n",
    "f = open('rel_PIS_USS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PIS_USS_ISS, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open('PPS.pkl', 'w')\n",
    "pickle.dump(ranking_PPS, f)\n",
    "f.close()\n",
    "f = open('PPS_USS.pkl', 'w')\n",
    "pickle.dump(ranking_PPS_USS, f)\n",
    "f.close()\n",
    "f = open('PPS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_PPS_ISS, f)\n",
    "f.close()\n",
    "f = open('PPS_USS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_PPS_USS_ISS, f)\n",
    "f.close()\n",
    "f = open('rel_PPS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PPS, f)\n",
    "f.close()\n",
    "f = open('rel_PPS_USS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PPS_USS, f)\n",
    "f.close()\n",
    "f = open('rel_PPS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PPS_ISS, f)\n",
    "f.close()\n",
    "f = open('rel_PPS_USS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PPS_USS_ISS, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open('PORS.pkl', 'w')\n",
    "pickle.dump(ranking_PORS, f)\n",
    "f.close()\n",
    "f = open('PORS_USS.pkl', 'w')\n",
    "pickle.dump(ranking_PORS_USS, f)\n",
    "f.close()\n",
    "f = open('PORS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_PORS_ISS, f)\n",
    "f.close()\n",
    "f = open('PORS_USS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_PORS_USS_ISS, f)\n",
    "f.close()\n",
    "f = open('rel_PORS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PORS, f)\n",
    "f.close()\n",
    "f = open('rel_PORS_USS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PORS_USS, f)\n",
    "f.close()\n",
    "f = open('rel_PORS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PORS_ISS, f)\n",
    "f.close()\n",
    "f = open('rel_PORS_USS_ISS.pkl', 'w')\n",
    "pickle.dump(ranking_rel_PORS_USS_ISS, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ml100k_rankings.pkl','r') as f:\n",
    "    l,ranking_PIS,ranking_PPS,ranking_PORS,ranking_USS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranking_USS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
